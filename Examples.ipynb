{
 "metadata": {
  "name": "",
  "signature": "sha256:4bc8fc6e13c2edad39690b3eeb25fa23c81e9044efe212b567f9d0e288284f2b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import pandas as pd\n",
      "import scipy\n",
      "import matplotlib.pyplot as plt\n",
      "from scipy.optimize import minimize, fmin_bfgs\n",
      "%matplotlib inline"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# load data\n",
      "movie_data = pd.io.excel.read_excel( open('moviedata_all.xlsx','r') )"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Given budget, you want to find some function that predicts gross_revenue such that (gross_prediction-gross_actual)**2 is minimized.\n",
      "# The simplest prediction function is something of the form f(budget)=c*budget .\n",
      "# If we thought there was a more complicated function that fit the data better, we could look at higher order polynomials but if\n",
      "#    we graph the data it looks like there's a linear correlation so that's what we'll start with\n",
      "# In train_func, we calculate the error given by theta[0] which is 'c' in the term np.sum( (gross-thetas[0]*budget)**2 )\n",
      "# Lambda*np.sum(thetas**2) does regularization which prevents overfitting (basically it means we don't want some crazy equation that\n",
      "#    fits the training data so well that it won't generalize to data not in the training set. It's not too important here but will be later.)\n",
      "# The error function is the raw error without the regularization term so we use this is how we test actual error. Also we don't have a\n",
      "#    sqrt in the train_func for a complicated reason involving ease of taking derivatives. It doesn't make a difference here, but it will later.\n",
      "\n",
      "\n",
      "def train_func(thetas, budget, studio, gross, Lambda):\n",
      "    return np.sum( (gross-thetas[0]*budget)**2 ) + Lambda*np.sum(thetas**2)\n",
      "\n",
      "def error(thetas, budget, studio, gross):\n",
      "    return np.sqrt( np.sum( (gross-thetas[0]*budget)**2 ) )\n",
      "\n",
      "best_lambda=0.1; best_err=1e10\n",
      "lambdas=[0.001,0.01,0.1,1,10,100,1e3,1e4]\n",
      "for Lambda in lambdas : # this loop looks at a bunch of different regularization weights to see which one is the best\n",
      "    train_data = movie_data.ix[:3100]\n",
      "    train_err=0; val_err=0\n",
      "    \n",
      "    # Given a validation weight, we now split the training data into validation and training data.\n",
      "    # The validation data is essentially test data (ie data the algo doesnt see in training) but we don't want to contaminate the \n",
      "    #    predictor by fiddling with the predictor based on data in the real test set so we need to have a validation set and a test set used later\n",
      "    for i in range(1) :\n",
      "        train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
      "        \n",
      "        # we convert each column of numeric data into standard deviation format\n",
      "        # this makes gradient descent and a lot of other algos run smoother because for example look at the derivative between \n",
      "        #   (1,1000) and (2,1010) . If one column contains most of it's meaningful information in the thousands digit and the \n",
      "        #   other in the unit digits, then a jump from 1 to 2 in x can be a lot larger in meaning than a jump in y from 1000 to 1010\n",
      "        #   so when we descend along gradients in gradient descent, we end up with higher noise if we don't normalize\n",
      "        budget = np.array(train_data['budget'])\n",
      "        budget_avg=np.average(budget); budget_std=np.std(budget)\n",
      "        budget=(budget-budget_avg)/budget_std\n",
      "        gross = np.array(train_data['total gross'])\n",
      "        gross_avg=np.average(gross); gross_std=np.std(gross)\n",
      "        gross=(gross-np.average(gross))/np.std(gross)\n",
      "        studio=np.array(train_data['studio'])\n",
      "        \n",
      "        # this solves the problem with .ix , instead we just split each individual 1d array\n",
      "        train_budget=budget[:2500]; val_budget=budget[2500:]; \n",
      "        train_gross=gross[:2500]; val_gross=gross[2500:]; \n",
      "        train_studio=studio[:2500]; val_studio=studio[2500:]; \n",
      "        \n",
      "        #train by gradient descent\n",
      "        initial_values = np.array([1])\n",
      "        learned_weights = scipy.optimize.fmin_bfgs(train_func, initial_values, maxiter=400, args=(train_budget, train_studio, train_gross, Lambda))\n",
      "\n",
      "        #compute error\n",
      "        train_err += error(learned_weights,train_budget,train_studio,train_gross)/len(train_gross)\n",
      "        val_err += error(learned_weights,val_budget,val_studio,val_gross)/len(val_gross)\n",
      "        \n",
      "    print Lambda, train_err, val_err\n",
      "    if val_err<best_err :\n",
      "        best_err=val_err; best_lambda=Lambda; best_weights=learned_weights\n",
      "print best_lambda\n",
      "\n",
      "# Now we take our entirely separate data that the algorithm hasn't seen before. This is the real indication of how the algorithm performs.\n",
      "# The goal is always to get the highest test results.\n",
      "# Also there's an interesting piece of theory that if there's a large gap between test and training results then we can improve the test\n",
      "#   results in many cases by decreasing how well the function(often called the parameterization) fits the training data. So basically\n",
      "#   we can reduce overfitting by making the regularization term larger or doing similar things in more complicated algorithms.\n",
      "test_budget = np.array(movie_data['budget'])[3100:]\n",
      "test_budget=(test_budget-budget_avg)/budget_std\n",
      "test_gross = np.array(movie_data['total gross'])[3100:]\n",
      "test_gross=(test_gross-gross_avg)/gross_std\n",
      "test_studio=np.array(movie_data['studio'])[3100:]\n",
      "\n",
      "test_err = error(np.array(best_weights),test_budget,test_studio,test_gross)/len(test_gross)\n",
      "print test_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Optimization terminated successfully.\n",
        "         Current function value: 1225.449779\n",
        "         Iterations: 2\n",
        "         Function evaluations: 12\n",
        "         Gradient evaluations: 4\n",
        "0.001 0.0140025672199 0.0311056237424\n",
        "Optimization terminated successfully.\n",
        "         Current function value: 1325.933684\n",
        "         Iterations: 2\n",
        "         Function evaluations: 12\n",
        "         Gradient evaluations: 4\n",
        "0.01 0.0145653225185 0.0262393965775\n",
        "Optimization terminated successfully.\n",
        "         Current function value: 1154.316158\n",
        "         Iterations: 3\n",
        "         Function evaluations: 15\n",
        "         Gradient evaluations: 5\n",
        "0.1 0.0135898122293 0.0341265031978\n",
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 1258.699920\n",
        "         Iterations: 3\n",
        "         Function evaluations: 117\n",
        "         Gradient evaluations: 35\n",
        "1 0.0141883965123 0.0296077957808\n",
        "Optimization terminated successfully.\n",
        "         Current function value: 1330.389381\n",
        "         Iterations: 2\n",
        "         Function evaluations: 12\n",
        "         Gradient evaluations: 4\n",
        "10 0.0145610208482 0.026352299319\n",
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 1324.318431\n",
        "         Iterations: 3\n",
        "         Function evaluations: 25\n",
        "         Gradient evaluations: 8\n",
        "100 0.0143032729489 0.0287151726692\n",
        "Optimization terminated successfully.\n",
        "         Current function value: 1493.570544\n",
        "         Iterations: 3\n",
        "         Function evaluations: 42\n",
        "         Gradient evaluations: 14\n",
        "1000.0 0.0141325714124 0.0356166686284\n",
        "Optimization terminated successfully.\n",
        "         Current function value: 2309.739210\n",
        "         Iterations: 2\n",
        "         Function evaluations: 12\n",
        "         Gradient evaluations: 4\n",
        "10000.0 0.0183733530805 0.0348192414434\n",
        "0.01\n",
        "0.0454721729829\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This one is the same as above except this time we're predicting f(budget,studio). Studio is slightly trickier because it's\n",
      "# not real-valued. Instead it's a member of a set, so what we want to do is for each possible studio we want to assign a bias weight\n",
      "# so maybe studio A on average causes a deviation of 2 from the prediction from budget, but studio B causes an average deviation of -4.\n",
      "# There's a quick heuristic for this if we train a classifier on just budget then train on studios on top of that, which is decently\n",
      "# faster and we don't lose much precision in practice, but this way is more extendable so I'm going to leave it like this for now. Feel\n",
      "# free to experiment with the independent training method though if it interests you.\n",
      "# TODO I think there's an error in here in the gradient descent because it should work better than the last method, so I need to fix that.\n",
      "#    (the precision loss warning in the results is ominous)\n",
      "\n",
      "# we want to create a mapping of studio name to an id so that we can reference the weights by integer ids (makes stuff cleaner later)\n",
      "studio=np.array(movie_data['studio'])        \n",
      "s=set(studio)\n",
      "studio_ids=dict([])\n",
      "i=0\n",
      "for item in s :\n",
      "    studio_ids[str(item)]=i; i+=1\n",
      "    \n",
      "def train_func(thetas, budget, studio, gross, Lambda):\n",
      "    studio_weights = [ thetas[ studio_ids[str(s)] ] for s in studio ]\n",
      "    budget_weight = thetas[61]\n",
      "    return np.sum( (gross-budget_weight*budget-studio_weights)**2 ) + Lambda*np.sum(thetas**2)\n",
      "\n",
      "def error(thetas, budget, studio, gross):\n",
      "    studio_weights = [ thetas[ studio_ids[str(s)] ] for s in studio ]\n",
      "    budget_weight = thetas[61]\n",
      "    return np.sqrt( np.sum( (gross-budget_weight*budget-studio_weights)**2 ) )\n",
      "\n",
      "best_lambda=0.1; best_err=1e10\n",
      "lambdas=[0.001,0.01,0.1,1,10,100,1e3,1e4]\n",
      "for Lambda in lambdas : # this loop looks at a bunch of different regularization weights to see which one is the best\n",
      "    train_data = movie_data.ix[:3100]\n",
      "    train_err=0; val_err=0\n",
      "    \n",
      "    # Given a validation weight, we now split the training data into validation and training data.\n",
      "    # The validation data is essentially test data (ie data the algo doesnt see in training) but we don't want to contaminate the \n",
      "    #    predictor by fiddling with the predictor based on data in the real test set so we need to have a validation set and a test set used later\n",
      "    for i in range(1) :\n",
      "        train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
      "        \n",
      "        # we convert each column of numeric data into standard deviation format\n",
      "        # this makes gradient descent and a lot of other algos run smoother because for example look at the derivative between \n",
      "        #   (1,1000) and (2,1010) . If one column contains most of it's meaningful information in the thousands digit and the \n",
      "        #   other in the unit digits, then a jump from 1 to 2 in x can be a lot larger in meaning than a jump in y from 1000 to 1010\n",
      "        #   so when we descend along gradients in gradient descent, we end up with higher noise if we don't normalize\n",
      "        budget = np.array(train_data['budget'])\n",
      "        budget_avg=np.average(budget); budget_std=np.std(budget)\n",
      "        budget=(budget-budget_avg)/budget_std\n",
      "        gross = np.array(train_data['total gross'])\n",
      "        gross_avg=np.average(gross); gross_std=np.std(gross)\n",
      "        gross=(gross-np.average(gross))/np.std(gross)\n",
      "        studio=np.array(train_data['studio'])\n",
      "        \n",
      "        # this solves the problem with .ix , instead we just split each individual 1d array\n",
      "        train_budget=budget[:2500]; val_budget=budget[2500:]; \n",
      "        train_gross=gross[:2500]; val_gross=gross[2500:]; \n",
      "        train_studio=studio[:2500]; val_studio=studio[2500:]; \n",
      "        \n",
      "        #train by gradient descent\n",
      "        initial_values = np.array([1]*62)\n",
      "        learned_weights = scipy.optimize.fmin_bfgs(train_func, initial_values, maxiter=400, args=(train_budget, train_studio, train_gross, Lambda))\n",
      "\n",
      "        #compute error\n",
      "        train_err += error(learned_weights,train_budget,train_studio,train_gross)/len(train_gross)\n",
      "        val_err += error(learned_weights,val_budget,val_studio,val_gross)/len(val_gross)\n",
      "        \n",
      "    print Lambda, train_err, val_err\n",
      "    if val_err<best_err :\n",
      "        best_err=val_err; best_lambda=Lambda; best_weights=learned_weights\n",
      "print best_lambda\n",
      "\n",
      "# Now we take our entirely separate data that the algorithm hasn't seen before. This is the real indication of how the algorithm performs.\n",
      "# The goal is always to get the highest test results.\n",
      "# Also there's an interesting piece of theory that if there's a large gap between test and training results then we can improve the test\n",
      "#   results in many cases by decreasing how well the function(often called the parameterization) fits the training data. So basically\n",
      "#   we can reduce overfitting by making the regularization term larger or doing similar things in more complicated algorithms.\n",
      "test_budget = np.array(movie_data['budget'])[3100:]\n",
      "test_budget=(test_budget-budget_avg)/budget_std\n",
      "test_gross = np.array(movie_data['total gross'])[3100:]\n",
      "test_gross=(test_gross-gross_avg)/gross_std\n",
      "test_studio=np.array(movie_data['studio'])[3100:]\n",
      "\n",
      "test_err = error(np.array(best_weights),test_budget,test_studio,test_gross)/len(test_gross)\n",
      "print test_err"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Warning: Desired error not necessarily achieved due to precision loss.\n",
        "         Current function value: 1074.305045\n",
        "         Iterations: 48\n",
        "         Function evaluations: 4737\n",
        "         Gradient evaluations: 74\n",
        "0.001 0.0131105881906 0.034639626969\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 1191.802504\n",
        "         Iterations: 51\n",
        "         Function evaluations: 4417\n",
        "         Gradient evaluations: 69\n",
        "0.01 0.0138086410373 0.0296175146266\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 1214.533278\n",
        "         Iterations: 54\n",
        "         Function evaluations: 4289\n",
        "         Gradient evaluations: 67\n",
        "0.1 0.013937217556 0.028383608381\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 1211.461332\n",
        "         Iterations: 52\n",
        "         Function evaluations: 4417\n",
        "         Gradient evaluations: 69\n",
        "1 0.013909319592 0.0288830402265\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 1270.254913\n",
        "         Iterations: 63\n",
        "         Function evaluations: 5761\n",
        "         Gradient evaluations: 90\n",
        "10 0.0141783443758 0.0266736571327\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "         Current function value: 1380.079555\n",
        "         Iterations: 77\n",
        "         Function evaluations: 7617\n",
        "         Gradient evaluations: 119\n",
        "100 0.0145858813386 0.0239086730802\n",
        "Warning: Desired error not necessarily achieved due to precision loss."
       ]
      }
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}